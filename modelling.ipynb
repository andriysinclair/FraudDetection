{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/as3620/.kaggle/kaggle.json'\n",
      "/home/as3620/FODS_coursework/LoanPrediction/data\n",
      "/home/as3620/FODS_coursework/LoanPrediction\n"
     ]
    }
   ],
   "source": [
    "# importing external libraries\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import set_config\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "set_config(transform_output = \"pandas\")\n",
    "\n",
    "# Importing function to load data\n",
    "\n",
    "# Making sure any changes are instantly added\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Modules.load_data import load_data\n",
    "from Modules.preprocessing import missing_summary, merge_dfs, dollar_to_int, find_unique_values\n",
    "from Modules.plotting import Plotter\n",
    "from Modules.transforming import *\n",
    "from Modules.modelling import MLearner\n",
    "\n",
    "# Importing Pipelines\n",
    "from Modules.Pipelines import Pipeline1, Pipeline2, Pipeline3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Root dir\n",
    "\n",
    "root = str(Path.cwd())\n",
    "\n",
    "# Obtaining seed from config.yaml\n",
    "\n",
    "# Load the config file\n",
    "with open(root + \"/config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "seed = config[\"global\"][\"seed\"]\n",
    "\n",
    "#print(f\"seed: {seed}\")\n",
    "\n",
    "# Set global seeds for reproducibility\n",
    "random.seed(seed)        \n",
    "np.random.seed(seed)     \n",
    "\n",
    "# Use the seed in scikit-learn\n",
    "random_state = check_random_state(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining absolute path to data folder\n",
    "\n",
    "data_folder = str(Path(os.getcwd()) / \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from pickle\n",
    "\n",
    "merged_df = pd.read_pickle(data_folder + \"/merged_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining params for CV grid search\n",
    "\n",
    "params_simple = {\"penalty\": [None], \"solver\":[\"saga\"], \"class_weight\": [\"balanced\"], \"max_iter\":[1000]}\n",
    "\n",
    "params_enhanced = [\n",
    "    {\"penalty\": [None], \"solver\":[\"saga\"], \"class_weight\": [None, \"balanced\"]},\n",
    "    {\"penalty\": [\"elasticnet\"], \"l1_ratio\" : np.linspace(0,1,10).tolist(), \n",
    "     \"C\": np.linspace(0.01,1,10).tolist(), \"solver\":[\"saga\"], \"class_weight\": [\"balanced\"], \"max_iter\":[1000]}    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use the simple parametrs to save computational time and answer the following questions:\n",
    "1. Is Pipeline2 better than Pipeline1. Is better granularity for date-like columns more predictive?\n",
    "2. Does using a dataset with a larger amount of \"Non-fraudulent transactions\" improve performance? As dataset is very imbalanced.\n",
    "\n",
    "Then we will carry out hyperparameter tuning on GLM and LGBM estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline1\n",
    "\n",
    "Transforming date into a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of fraudulent transactions in y_train: 0.1302501302761855\n",
      "% of fraudulent transactions in y_test: 0.1302954509926528\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.897 total time=   0.6s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.888 total time=   0.6s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.901 total time=   0.6s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.894 total time=   0.6s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.895 total time=   0.6s\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.8952503091650255\n",
      "score on testing set: 0.8457047539616347\n"
     ]
    }
   ],
   "source": [
    "ML_pipe1 = MLearner(dataset=merged_df, transformation_pipeline=Pipeline1, params=params_simple, estimator=LogisticRegression(random_state=seed))\n",
    "ML_pipe1.fit()\n",
    "ML_pipe1.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline2\n",
    "\n",
    "Decomposing date-like features into hours, weeks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of fraudulent transactions in y_train: 0.1302501302761855\n",
      "% of fraudulent transactions in y_test: 0.1302954509926528\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.905 total time=   0.7s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.904 total time=   0.7s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.915 total time=   0.7s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.905 total time=   0.7s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.914 total time=   0.7s\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.909183531700088\n",
      "score on testing set: 0.8632022471910112\n"
     ]
    }
   ],
   "source": [
    "ML_pipe2 = MLearner(dataset=merged_df, transformation_pipeline=Pipeline2, params=params_simple, estimator=LogisticRegression(random_state=seed))\n",
    "ML_pipe2.fit()\n",
    "ML_pipe2.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3\n",
    "\n",
    "As we can see the date transformations improve performance. We will now see what an effect increasing the size of the dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine learning with 0.00125 of dataset...\n",
      "\n",
      "% of fraudulent transactions in y_train: 0.5446467509812473\n",
      "% of fraudulent transactions in y_test: 0.5463614063777597\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.989 total time=   1.2s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.990 total time=   1.4s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.988 total time=   1.3s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.985 total time=   1.4s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.986 total time=   1.4s\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.9878182725911133\n",
      "score on testing set: 0.9453357100415924\n",
      "\n",
      "Machine learning with 0.0025 of dataset...\n",
      "\n",
      "% of fraudulent transactions in y_train: 0.37202592828506126\n",
      "% of fraudulent transactions in y_test: 0.3824884792626728\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.976 total time=   1.2s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.979 total time=   1.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.976 total time=   1.1s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.974 total time=   1.1s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.975 total time=   1.1s\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.9756437714798027\n",
      "score on testing set: 0.9361950649729888\n",
      "\n",
      "Machine learning with 0.005 of dataset...\n",
      "\n",
      "% of fraudulent transactions in y_train: 0.23121254034117106\n",
      "% of fraudulent transactions in y_test: 0.22835408022130013\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.948 total time=   0.9s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.956 total time=   0.9s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.956 total time=   0.9s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.954 total time=   1.1s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.954 total time=   0.9s\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.9537569461222517\n",
      "score on testing set: 0.9098470221298084\n",
      "\n",
      "Machine learning with 0.01 of dataset...\n",
      "\n",
      "% of fraudulent transactions in y_train: 0.12950586886570004\n",
      "% of fraudulent transactions in y_test: 0.1325282369953492\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.909 total time=   0.7s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.911 total time=   0.7s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.903 total time=   0.6s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.909 total time=   0.7s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.900 total time=   0.7s\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.9065724644417589\n",
      "score on testing set: 0.8664647577092511\n",
      "\n",
      "Machine learning with 0.05 of dataset...\n",
      "\n",
      "% of fraudulent transactions in y_train: 0.029213894924216645\n",
      "% of fraudulent transactions in y_test: 0.028690098076855956\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.560 total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.564 total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.576 total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.610 total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END class_weight=balanced, max_iter=1000, penalty=None, solver=saga;, score=0.604 total time= 1.2min\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.5567271907071163\n",
      "score on testing set: 0.5220726095108232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up logging configuration\n",
    "\n",
    "reduction_p = [0.00125, 0.0025, 0.005, 0.01, 0.05]\n",
    "\n",
    "for p in reduction_p:\n",
    "    print(f\"\\nMachine learning with {p} of dataset...\\n\")\n",
    "    merged_df3_copy = Target0_Reducer(percentage=p).fit_transform(merged_df)\n",
    "\n",
    "    ML_pipe3 = MLearner(dataset=merged_df3_copy, transformation_pipeline=Pipeline3, params=params_simple, estimator=LogisticRegression(random_state=seed), scoring=\"f1\")\n",
    "    ML_pipe3.fit()\n",
    "    ML_pipe3.predict()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly we see that as we increase the dataframe size performance decreases. Perhaps the model cannot generalise well to so few fraudulent transactions. In addition Logit fails to converge. We propose to use a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:X_train indices: Index([19973, 18804, 26056, 23547, 23256, 11170, 20521,  3638,  6547,  7827,\n",
      "       ...\n",
      "       16850,  6265, 22118, 11284, 11964, 21575,  5390,   860, 15795, 23654],\n",
      "      dtype='int64', length=19998)\n",
      "DEBUG:root:X_test indices: Index([20966, 26365, 20166,  8763,  7335, 25135, 18791, 12228, 11812, 19142,\n",
      "       ...\n",
      "        8390, 17202,  6161,  7801, 26575,  3388,  3310, 23973, 18548, 24660],\n",
      "      dtype='int64', length=6666)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of fraudulent transactions in y_train: 0.5011501150115012\n",
      "% of fraudulent transactions in y_test: 0.4965496549654965\n",
      "\n",
      "Best parameters found: {'class_weight': 'balanced', 'max_iter': 1000, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.9849840891010342\n",
      "score on testing set: 0.9477623110312827\n"
     ]
    }
   ],
   "source": [
    "# Obtaining score for balanced class\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "\n",
    "merged_df3_copy = Target0_Reducer(balanced=True).fit_transform(merged_df)\n",
    "\n",
    "ML_pipe3 = MLearner(dataset=merged_df3_copy, transformation_pipeline=Pipeline3, params=params_simple, estimator=LogisticRegression(), scoring=\"f1\")\n",
    "ML_pipe3.fit()\n",
    "ML_pipe3.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:X_train indices: Index([18137, 19765, 18101, 26603,  3625, 16291,  5454, 12365, 20777, 15780,\n",
      "       ...\n",
      "        5566,   284, 23527,  5093, 14301, 15058, 25230, 12134,  2082,  7590],\n",
      "      dtype='int64', length=19998)\n",
      "DEBUG:root:X_test indices: Index([18988, 24283,  4969, 12047, 23953,   812, 23493, 10565, 20631, 22202,\n",
      "       ...\n",
      "        5582, 12400,  4382, 16181, 26619,  6945,  2818,  5969, 21674, 21480],\n",
      "      dtype='int64', length=6666)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of fraudulent transactions in y_train: 0.502050205020502\n",
      "% of fraudulent transactions in y_test: 0.49384938493849384\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'class_weight': None, 'penalty': None, 'solver': 'saga'}\n",
      "score on training set: 0.9857485748574858\n",
      "score on testing set: 0.9426942694269427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as3620/miniconda3/envs/FODS_coursework/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Now hyperparameter tuning\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "\n",
    "merged_df3_copy = Target0_Reducer(balanced=True).fit_transform(merged_df)\n",
    "\n",
    "ML_pipe3 = MLearner(dataset=merged_df3_copy, transformation_pipeline=Pipeline3, params=params, estimator=LogisticRegression(), scoring=\"accuracy\")\n",
    "ML_pipe3.fit()\n",
    "ML_pipe3.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM set up with Pipeline3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of fraudulent transactions in y_train: 0.5025502550255025\n",
      "% of fraudulent transactions in y_test: 0.49234923492349236\n",
      "\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6700, number of negative: 6632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1703\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502550 -> initscore=0.010201\n",
      "[LightGBM] [Info] Start training from score 0.010201\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=50;, score=0.988 total time=  45.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6700, number of negative: 6632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1699\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502550 -> initscore=0.010201\n",
      "[LightGBM] [Info] Start training from score 0.010201\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=50;, score=0.982 total time=  43.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6700, number of negative: 6632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1703\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502550 -> initscore=0.010201\n",
      "[LightGBM] [Info] Start training from score 0.010201\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=50;, score=0.986 total time=  44.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6700, number of negative: 6632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1703\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502550 -> initscore=0.010201\n",
      "[LightGBM] [Info] Start training from score 0.010201\n",
      "[CV 1/3] END learning_rate=0.02, n_estimators=50;, score=0.990 total time=  45.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6700, number of negative: 6632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1699\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502550 -> initscore=0.010201\n",
      "[LightGBM] [Info] Start training from score 0.010201\n",
      "[CV 2/3] END learning_rate=0.02, n_estimators=50;, score=0.985 total time=  46.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6700, number of negative: 6632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1703\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502550 -> initscore=0.010201\n",
      "[LightGBM] [Info] Start training from score 0.010201\n",
      "[CV 3/3] END learning_rate=0.02, n_estimators=50;, score=0.986 total time=  43.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 10050, number of negative: 9948\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1704\n",
      "[LightGBM] [Info] Number of data points in the train set: 19998, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502550 -> initscore=0.010201\n",
      "[LightGBM] [Info] Start training from score 0.010201\n",
      "Best parameters found: {'learning_rate': 0.02, 'n_estimators': 50}\n",
      "score on training set: 0.9911491149114912\n",
      "score on testing set: 0.9273927392739274\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# LGBM params\n",
    "\n",
    "lgbm_params =     {\"learning_rate\": [0.01, 0.02],\n",
    "                   \"n_estimators\": [50],\n",
    "                   #\"reg_alpha\": np.linspace(0,1,10).tolist(),\n",
    "                   #\"reg_lambda\": np.linspace(0,1,10).tolist(),\n",
    "                   }\n",
    "\n",
    "\n",
    "merged_lgbm = Target0_Reducer(balanced=True).fit_transform(merged_df)\n",
    "\n",
    "ML_pipe3 = MLearner(dataset=merged_lgbm, transformation_pipeline=Pipeline3, params=lgbm_params, estimator=LGBMClassifier(), scoring=\"accuracy\",cv=3)\n",
    "ML_pipe3.fit()\n",
    "ML_pipe3.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of fraudulent transactions in y_train: 0.5011501150115012\n",
      "% of fraudulent transactions in y_test: 0.4965496549654965\n",
      "\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6682, number of negative: 6650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501200 -> initscore=0.004800\n",
      "[LightGBM] [Info] Start training from score 0.004800\n",
      "[CV 1/3] END learning_rate=0.02, n_estimators=50;, score=0.987 total time=  43.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1716\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 2/3] END learning_rate=0.02, n_estimators=50;, score=0.986 total time=  44.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1717\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 3/3] END learning_rate=0.02, n_estimators=50;, score=0.986 total time=  43.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6682, number of negative: 6650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501200 -> initscore=0.004800\n",
      "[LightGBM] [Info] Start training from score 0.004800\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=50;, score=0.989 total time=  47.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1716\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=50;, score=0.991 total time=  48.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1717\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=50;, score=0.991 total time=  45.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 10022, number of negative: 9976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 19998, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501150 -> initscore=0.004600\n",
      "[LightGBM] [Info] Start training from score 0.004600\n",
      "Best parameters found: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "score on training set: 0.9973497349734973\n",
      "score on testing set: 0.9539453945394539\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lgbm_params =     {\"learning_rate\": [0.02, 0.1],\n",
    "                   \"n_estimators\": [50],\n",
    "                   #\"reg_alpha\": np.linspace(0,1,10).tolist(),\n",
    "                   #\"reg_lambda\": np.linspace(0,1,10).tolist(),\n",
    "                   }\n",
    "\n",
    "\n",
    "merged_lgbm = Target0_Reducer(balanced=True).fit_transform(merged_df)\n",
    "\n",
    "ML_pipe3 = MLearner(dataset=merged_lgbm, transformation_pipeline=Pipeline3, params=lgbm_params, estimator=LGBMClassifier(), scoring=\"accuracy\",cv=3)\n",
    "ML_pipe3.fit()\n",
    "ML_pipe3.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of fraudulent transactions in y_train: 0.5011501150115012\n",
      "% of fraudulent transactions in y_test: 0.4965496549654965\n",
      "\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6682, number of negative: 6650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501200 -> initscore=0.004800\n",
      "[LightGBM] [Info] Start training from score 0.004800\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=50;, score=0.989 total time=  47.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1716\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=50;, score=0.991 total time=  47.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1717\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=50;, score=0.991 total time=  47.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6682, number of negative: 6650\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501200 -> initscore=0.004800\n",
      "[LightGBM] [Info] Start training from score 0.004800\n",
      "[CV 1/3] END learning_rate=0.2, n_estimators=50;, score=0.992 total time=  46.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1716\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 2/3] END learning_rate=0.2, n_estimators=50;, score=0.991 total time=  47.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6681, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1717\n",
      "[LightGBM] [Info] Number of data points in the train set: 13332, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501125 -> initscore=0.004500\n",
      "[LightGBM] [Info] Start training from score 0.004500\n",
      "[CV 3/3] END learning_rate=0.2, n_estimators=50;, score=0.992 total time=  50.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 10022, number of negative: 9976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 19998, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501150 -> initscore=0.004600\n",
      "[LightGBM] [Info] Start training from score 0.004600\n",
      "Best parameters found: {'learning_rate': 0.2, 'n_estimators': 50}\n",
      "score on training set: 1.0\n",
      "score on testing set: 0.9558955895589559\n"
     ]
    }
   ],
   "source": [
    "lgbm_params =     {\"learning_rate\": [0.1, 0.2],\n",
    "                   \"n_estimators\": [50],\n",
    "                   #\"reg_alpha\": np.linspace(0,1,10).tolist(),\n",
    "                   #\"reg_lambda\": np.linspace(0,1,10).tolist(),\n",
    "                   }\n",
    "\n",
    "\n",
    "merged_lgbm = Target0_Reducer(balanced=True).fit_transform(merged_df)\n",
    "\n",
    "ML_pipe3 = MLearner(dataset=merged_lgbm, transformation_pipeline=Pipeline3, params=lgbm_params, estimator=LGBMClassifier(), scoring=\"accuracy\",cv=3)\n",
    "ML_pipe3.fit()\n",
    "ML_pipe3.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Target0_Reducer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m lgbm_params \u001b[38;5;241m=\u001b[39m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.2\u001b[39m],\n\u001b[1;32m      2\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m      3\u001b[0m                    \u001b[38;5;66;03m#\"reg_alpha\": np.linspace(0,1,10).tolist(),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                    \u001b[38;5;66;03m#\"reg_lambda\": np.linspace(0,1,10).tolist(),\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                    }\n\u001b[0;32m----> 8\u001b[0m merged_lgbm \u001b[38;5;241m=\u001b[39m \u001b[43mTarget0_Reducer\u001b[49m(balanced\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit_transform(merged_df)\n\u001b[1;32m     10\u001b[0m ML_pipe3 \u001b[38;5;241m=\u001b[39m MLearner(dataset\u001b[38;5;241m=\u001b[39mmerged_lgbm, transformation_pipeline\u001b[38;5;241m=\u001b[39mPipeline3, params\u001b[38;5;241m=\u001b[39mlgbm_params, estimator\u001b[38;5;241m=\u001b[39mLGBMClassifier(), scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     11\u001b[0m ML_pipe3\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Target0_Reducer' is not defined"
     ]
    }
   ],
   "source": [
    "lgbm_params =     {\"learning_rate\": [0.2],\n",
    "                   \"n_estimators\": [100],\n",
    "                   #\"reg_alpha\": np.linspace(0,1,10).tolist(),\n",
    "                   #\"reg_lambda\": np.linspace(0,1,10).tolist(),\n",
    "                   }\n",
    "\n",
    "\n",
    "merged_lgbm = Target0_Reducer(balanced=True).fit_transform(merged_df)\n",
    "\n",
    "ML_pipe3 = MLearner(dataset=merged_lgbm, transformation_pipeline=Pipeline3, params=lgbm_params, estimator=LGBMClassifier(), scoring=\"accuracy\",cv=3)\n",
    "ML_pipe3.fit()\n",
    "ML_pipe3.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FODS_coursework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
